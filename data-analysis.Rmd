---
title: Traffic Data Analysis
output: html_document
---

```{r}
library(ggplot2)
library(dplyr)
library(knitr)
library(tidyr)
library(reshape2)
```

# Data Processing

## Initialization

We first load our CSV file and take a glimpse at its contents.

```{r}
data.raw <- read.csv('all-semi-unique.csv')
glimpse(data.raw)
```

At first glance, we see that our raw data is composed of approximately 430k observations of 34 variables. Let us proceed to clean this data.

## Advertising data removal

Knowing that variables prefixed with **ad.** contain information relevant only to the application's advertising logic, we may safely drop them as they will not be used in our investigation.

```{r}
data.road <- data.raw %>%
  select(-(starts_with("ad.")))
```

## Removing constant valued columns

Now that we have dropped variables that will not be of interest, we can investigate for variables that always carry a constant value in our observations

```{r}
data.road %>%
  sapply(unique) %>%
  sapply(length)
```

We can spot two variables that are always constant: **rd.cl** and **rd.rp.type**. Since they add no information to our data, we may safely discard them. We then take note of the format the *crawl_date* variable is in.

```{r}
data.road <- data.road %>%
  select(-(rd.cl), -(rd.rp.type))
head(data.road$crawl_date)
```

## Crawl date format

The crawl_date column currently contains a factor of different strings. We proceed to parse this into a more machine readable format as a column of time values.

```{r}
data.road$crawl_date <-
  strptime(data.road$crawl_date, format = "%a %b %e %X UTC %Y", tz = "UTC") %>%
  as.POSIXct()
head(data.road$crawl_date)
```

## Report submission time estimation

Upon further inspection of bey2ollak.com's response data: **rd.rp.hr** and **rd.rp.mn** are the time differences between the crawl date and the report submission time. This means we can estimate the time of the response submission accurately down to a minute, which should be sufficient for our purposes. 

```{r}
data.road <- data.road %>%
  mutate(report_time = as.POSIXct(round(crawl_date - (rd.rp.hr*60*60 + rd.rp.mn*60), "mins"))) %>%
  select(-c(rd.rp.mn, rd.rp.hr))
```

## Road status update time estimation

**rd.hr** and **rd.mn** reflect how much time has passed since a road's status was updated on bey2ollak's end. We can extrapolate the last time a road's status was updated using similar means.

```{r}
data.road <- data.road %>%
  mutate(last_road_update = as.POSIXct(round(crawl_date - (rd.hr*60*60 + rd.mn*60), "mins"))) %>%
  select(-c(rd.mn, rd.hr))
```

## Road name split

We note that **rd.nm** contains a combination of the major road name and the minor road name. Splitting this column into two separate columns will help clarify more information about major roads and their minor segments.

```{r}
data.road <- data.road %>%
  separate(rd.nm, c("rd.majornm", "rd.minornm"), ";")
```

Some reports belong to a major road name as a whole with no minor road name. We proceed to replace the missing minor road names with a standard value.

```{r}
data.road$rd.minornm[is.na(data.road$rd.minornm)] <- "NO_MINOR"
```

We will now migrate the road names data to a different data frame in order to separate concerns.

```{r}
data.road_names <- data.road %>%
  select(rd.ri, rd.majornm, rd.minornm) %>%
  unique()
num_ids <- data.road_names %>%
  select(rd.ri) %>%
  unique() %>%
  nrow()
num_entries <- data.road_names %>% nrow()
num_ids == num_entries # # check for problems, expect TRUE
rm(num_ids, num_entries) # cleanup env

data.road <- data.road %>%
  select(-c(rd.majornm, rd.minornm))
```

## Extracting road status snapshots

Each crawl of the data represents a snapshot of all the road statuses at that time if we take the right perspective. Since every entry in our data is the road status data appended to some report data, we can isolate the road status information for analysis.

First, let us ensure that we have only a single rd.stid for each road per crawl.

```{r}
numberOfTripletsWithSTID <- data.road %>%
  select(crawl_date, rd.ri, rd.stid) %>%
  unique() %>%
  nrow()
numberOfTriplets <- data.road %>%
  select(crawl_date, rd.ri) %>%
  unique() %>%
  nrow()
numberOfTriplets == numberOfTripletsWithSTID # check for problems, expect TRUE
rm(numberOfTriplets, numberOfTripletsWithSTID) # cleanup env
```

We can now proceed to isolate road status snapshots knowing that taking unique pairs will not result in information loss.

```{r}
data.road.status <- data.road %>%
  select(crawl_date, rd.ri, rd.stid) %>%
  unique()
table(data.road.status$rd.stid)
```

Unfortunately, due to the way the data was crawled, there is an overwhelming amount of rows with **rd.stid** equal to 10 (info). This destroys the information conveyed by bey2olak about the road and renders this approach useless unless we attempt to approximate the correct congestion levels.

We choose discard the polluted column and attempt to follow a different approach. As we have also chosen to disregard the "snapshots" of road status, we may also discard the **crawl_date** and **last_road_update** columns.

```{r}
rm(data.road.status)
data.road <- data.road %>%
  select(-c(rd.stid, crawl_date, last_road_update)) %>%
  unique()
```

## Application-specific data elimination

We choose to discard **rd.new**, **rd.strq**, **rd.cmrq** and **rd.img** as they play a role in bey2ollak's application functionality and will not provide traffic insight.

```{r}
data.road <- data.road %>%
  select(-c(rd.new, rd.strq, rd.cmrq, rd.img)) %>%
  unique()
```

## User information

We choose not to do any analysis on user profiles, such as inspecting wether users with profile images/full names tend to be more or less active, and drop both the **rd.rp.fullnm** and **rd.rp.img** columns.

```{r}
data.road <- data.road %>%
  select(-c(rd.rp.fullnm, rd.rp.img)) %>%
  unique()
```

## Report time estimate errors

We are now in the position to partially account for some duplicates that result due to the error margin present in our report time estimates. We can remedy this by ignoring the possible *off-by-one* differences in **report_time** estimates resulting from our accruacy being limited to ± 1 minute.

```{r}
rpt_dupes <- data.road %>%
  select(-(report_time)) %>%
  duplicated()
data.road <- data.road %>%
  subset(!rpt_dupes)
rm(rpt_dupes)
```

## Extracting travel speeds

We will now attempt to extrapolate travel speeds so we can use them as the main metric later on in our analysis.

First, we will note that bey2ollak's auto report feature uses the following categorization for speed ranges:

Classification  | Speed Range  | rd.rp.stid 
--------------- | ------------ | -----------
7alawa          | 80+          | 1
lazeez          | 40-79        | 2
mashy           | 20-39        | 3
za7ma           | 10-19        | 4
mafeesh amal    | 0-9          | 5

The following rd.rp.stid values also correspond to different report types:

Type          | rd.rp.stid
------------- | -----------
so2al         | 6
khatar        | 7
7adsa         | 8
3otl          | 9
ba2ollak eh   | 10

A report can include both a congestion rating (7alawa - mafeesh 2amal) and/or a khatar/7adsa/3otl warning, or a report can be a so2al/ba2ollak eh. Unfortunately, the data crawling eliminated multiple <stid> tags and some information was lost. We can extract the questions since it is reasonable to assume they do not contribute to congestion level information.

```{r}
data.questions <- data.road %>%
  filter(rd.rp.stid == 6)
data.road <- data.road %>%
  filter(rd.rp.stid != 6)
data.road %>% nrow()
```

We will proceed to initially use the above classifications to approximate the value of a new column we will introduce which will represent average travel speed.

```{r}
data.road <- data.road %>%
  mutate(speed = NA)
data.road$speed[data.road$rd.rp.stid == 1] <- 85
data.road$speed[data.road$rd.rp.stid == 2] <- 60
data.road$speed[data.road$rd.rp.stid == 3] <- 30
data.road$speed[data.road$rd.rp.stid == 4] <- 15
data.road$speed[data.road$rd.rp.stid == 5] <- 5
data.road %>% select(speed) %>% is.na() %>% sum()
```

The automatic reporter produces templated comments containing our desired values. We can use these to fine tune our speed column values.

```{r}
library(stringr)
regexp <- "(\\d+ km/h)|(\\d+ كم/س)"
matched <- grepl(regexp, data.road$rd.rp.cm)

subs <- data.road$rd.rp.cm %>%
  subset(matched)
matches <- regmatches(subs, regexpr(regexp, subs))
data.road$speed[matched] <- gsub("\\D", " ", matches) %>%
  str_trim(side = "both") %>%
  as.integer()

rm(regexp, matched, subs, matches)

data.road %>% select(speed) %>% is.na() %>% sum()
```

Now we can isolate "ba2ollak eh" reports that do not contain any speed information. Furthermore, as the remaining NA values are randomly distributed across report_times, and are small in quantity relative to the remaining observation count, we will discard them.

```{r}
data.info <- data.road %>%
  filter(rd.rp.stid == 10 & is.na(speed))
data.road <- data.road %>%
  filter(!is.na(speed))
```

## Isolating Speed Reports

```{r}
data.speed <- data.road %>%
  select(rd.ri, rd.rp.cmid, report_time, speed) %>%
  rename(road = rd.ri, comment = rd.rp.cmid) %>%
  unique()
```

# Descriptive analysis

We are now left with a clean set of observations of approximate average travel speeds across different roads at different times.

```{r}
glimpse(data.speed)
```

## Data distribution across time

Our main concern in this report is not application usage analysis, but it will be useful to develop some insight into the distribution of our data points across our crawling timespan. This will help us identify any gaps in our knowledge of speed data.

```{r}
bins <- data.speed %>%
  split(cut(data.speed$report_time, "hour"))
sizes <- sapply(bins, nrow)
plot(sizes, type = "l")
```

We can observe from the above plot that our data is quite sparse in the first few days and in the last day of our crawl range. Therefore, we will focus our analysis on the two weeks from 07/02 until 21/02, where our data is not assumed to have gaps.

```{r}
left_boundary <- as.POSIXct(strptime("2016-02-07 04:00:00", "%F %X"), tz = "UTC")
right_boundary <- as.POSIXct(strptime("2016-02-21 04:00:00", "%F %X"), tz = "UTC")
data.speed <- data.speed %>%
  filter(report_time >= left_boundary, report_time < right_boundary)
```

```{r}
bins <- data.speed %>%
  split(cut(data.speed$report_time, "hour"))
sizes <- sapply(bins, nrow)
plot(sizes, type = "l")
```

We can now see some app usage patterns across weekdays and weekends.

# The Ring Road

Even though our data fits in memory, we have a plethora of information. Traffic flow data, even when only recorded sparsely for a couple of cities over two weeks, represents the ongoings of a very complex system with many intricacies that can not be summed up in a few global metrics. Therefore, we will carry out a small practical analysis by isolating the ring road data, which forms a cohesive road that can be meshed together and examined for speed patterns.

ID   |   Segment
---- | ------------------------------
32   |          Moneeb To Autostrad
281  |     Autostrad To Sokhna Exit
315  |       Sokhna Exit To Tagamo3
316  |          Tagamo3 To Suez Rd.
302  |    Suez Rd. To Nafa2 ElSalam
285  |      Nafa2 ElSalam To Zera3y
286  |             Zera3y To Me7war
122  |   Me7war To Waslet Maryoutia
125  |   Waslet Maryoutia To Moneeb
---  |    **Opposite Direction**   
126  |   Moneeb To Waslet Maryoutia
121  |   Waslet Maryoutia To Me7war
283  |             Me7war To Zera3y
284  |      Zera3y To Nafa2 ElSalam
301  |    Nafa2 ElSalam To Suez Rd.
317  |          Suez Rd. To Tagamo3
318  |       Tagamo3 To Sokhna Exit
282  |     Sokhna Exit To Autostrad
 31  |          Autostrad To Moneeb

We will now define the analysis procedure that is to be carried out on both directions of the ring road. We have already checked the filtered data for consistency across the selected time period and acceptable results were found.

```{r}
ring_road     <- c(31, 282, 318, 317, 301, 284, 283, 121, 126)
ring_road_ccw <- c(32, 281, 315, 316, 302, 285, 286, 122, 125)

roadAnalysis <- function(segments, L = left_boundary, R = right_boundary) {
  focus <- data.speed %>%
    filter(report_time >= L, report_time < R)
  
  bins <- focus %>%
    split(cut(focus$report_time, "hour"))
  
  subs <- lapply(bins, function(bin) {
    tmp <- bin %>% 
      filter(road %in% segments) %>% 
      group_by(road) %>% 
      summarise(avg_spd = mean(speed))
    
    ord <- tmp[order(factor(tmp$road, levels = segments)),]
    
    dframe <- data.frame(matrix(0, ncol = nrow(tmp), nrow = 1))
    names(dframe) <- ord$road
    dframe[1,] <- ord$avg_spd
    return(dframe)
  })
  
  dframe <- rbind_all(subs)
  row.names(dframe) <- names(subs)
  names(dframe) <- 1:9
  dmatrix <- data.matrix(dframe)
  #hmap <- heatmap(dmatrix, Rowv=NA, Colv=NA, col = heat.colors(256), scale="column", margins=c(5,10))
  return(
    ggplot(melt(dmatrix), aes(Var2, Var1, fill = value)) + 
      geom_raster() + 
      scale_fill_continuous(na.value = 'steelblue'))
}
```

### Clockwise

```{r}
roadAnalysis(
  ring_road,
  as.POSIXct(strptime("2016-02-07 04:00:00", "%F %X"), tz = "UTC"),
  as.POSIXct(strptime("2016-02-08 00:00:00", "%F %X"), tz = "UTC"))
```

### Counter-clockwise

```{r}
roadAnalysis(ring_road_ccw)
```

## Speed tables

```{r}
speeds <- sapply(bins, function(x) {
    select(x, speed)
  })
avgs <- sapply(speeds, mean)
```

## Average Speed Correllations
Average speed of cars per road across time

Correlation of average speeds between roads
  
# Inferential analysis

# And beyond..

```{r}
glimpse(data.road)
```

rd.rp.cmid is the id of the comment. ri is the road id. img is the user profile image id, rpImg is the id of the image attached to the report if any. strq, I guess, refers to street request? it appears any road with only one direction or segment has this set to zero, while the others are set to one. cmrq might refer to comment request?

rd.rp.stid:
1   7alawa :D 80+
2   lazeez  :) 40-79
3   mashy :| 20-39
4   za7ma  :( 10-19
5   mafeesh 2amal :'( 9-
6   so2al ?
7   khatar X
8   7adsa =:=
9   3otl /\
10  ba2olak eh !

rd.stid:
